# Задача CodeX: Advanced Retrieval & Speed для закрытого RAG

## Кратко

Реализуйте улучшенный модуль **Advanced Retrieval** для закрытого корпуса документов c требованиями:

* гибридный поиск (dense+BM25) → **parent→page** расширение → **rerank**,
* **кэширование** и **батчинг** для ускорения,
* строгий **JSON** с валидными **ссылками (doc+page)**.

Работать нужно **офлайн**, без внешнего интернета и API. Скрипт запуска — `scripts/run_all.py`.

---

## Дано

Репозиторий со скелетом (минимальный baseline уже работает):

```
answer/         # генерация, валидация, проверка источников
configs/        # default.yaml
data/           # артефакты индекса и ответы
index/          # FAISS + BM25
ingest/         # парсинг PDF (PyMuPDF)
orchestrator/   # пайплайн, очереди
retriever/      # router, hybrid, parent_page, rerank (плейсхолдер)
scripts/run_all.py
```

Вход:

* `./corpus/*.pdf` — закрытый корпус (имя файла = doc_id.pdf).
* `./data/questions.jsonl` — по строке на вопрос:

  ```json
  {"question_id": 1, "question_text": "Текст вопроса"}
  ```

Выход:

* `./answers.json` — список объектов:

  ```json
  {
    "question_id": "<string|int>",
    "answer": "<number|boolean|string|array[string]|\"N/A\">",
    "sources": [{"document":"<doc_id>","page":<int>}]
  }
  ```

---

## Что нужно сделать

Реализовать четыре доработки (A–D). **Изменять можно только указанные файлы**; публичные сигнатуры не трогать.

### A) Реальные dense-эмбеддинги (вместо заглушки)

**Файл:** `index/build_dense.py`

* Заменить `dummy_embed(...)` на реальную функцию `embed(texts) -> np.ndarray` c **нормализацией** векторов.
* В офлайн-режиме допускается:

  1. локальная модель из установленного колеса (например, `sentence-transformers`), **если организаторы разрешают**;
  2. либо компактный собственный TF-IDF→SVD (deterministic) — без интернета, предобучение в рантайме на тексте страниц.
* Требование: индекс FAISS **с косинусной близостью** (через dot product по L2-нормированным векторам).

### B) Гибридный поиск + merge

**Файл:** `retriever/hybrid.py`

* Реализовать объединение результатов **dense top_k_dense** и **BM25 top_k_bm25**:

  * нормировка скорингов (min–max или по max/percentile),
  * взвешенная сумма `score = α*dense + (1-α)*bm25`, где `α` из конфига (если нет — принять `α=0.7`),
  * **dedup по (doc_id,page)**,
  * вернуть список отсортированных кандидатов **(не менее 24 по умолчанию)**.

### C) Rerank: cross-encoder/LLM-like (офлайн)

**Файл:** `retriever/rerank.py`

* Заменить плейсхолдер на rerank-функцию `rerank(query, pages, top_m)`:

  * допускается **легкий cross-encoder** (например, MiniLM) или **строгий BM25-на-подстроке** как суррогат CE,
  * **батчинг** входов (пакеты по 4–8 страниц на запрос),
  * стабильная шкала \[0..1] + топ `top_m` (из конфига).
* Цель: поднять **релевантную страницу в top-1** на тестовых “сложных” запросах (см. критерии ниже).

### D) Кэширование и батчинг в оркестраторе

**Файл:** `orchestrator/pipeline.py`

* Добавить:

  * `retrieval_cache` (LRU, размер ≥ 512 ключей) по ключу `hash(query_norm)`,
  * `rerank_cache` по `(query_hash, doc_id, page)`,
  * **асинхронный батчинг** вопросов (одновременные задачи ≥ 8),
  * логирование счётчиков: `cache_retrieval_hits`, `cache_rerank_hits`, `avg_latency_ms`.
* Не менять формат вывода.

---

## Ограничения

* **Никаких сетевых вызовов**.
* Разрешены стандартная библиотека + зависимости из `requirements.txt`.
  (Если в окружении доступен `sentence-transformers`/`torch` — можно использовать; иначе сделайте офлайн-реализацию SVD.)
* Время работы в CodeX: данные небольшие (десятки PDF), **200 вопросов**. Ожидается, что решение укладывается в лимит.

---

## Критерии прохождения (автотесты)

Тесты выполняются `python -m pytest` (внутри CodeX). Считается, что вы запускаете end-to-end через `scripts/run_all.py`.

1. **Строгий JSON**

   * 100% объектов валидны по схеме; тип `answer` корректен.

2. **Цитирование (valid sources)**

   * В ≥ 98% ответов хотя бы **одна указанная страница** содержит подтверждение ответа
     (строковый матч для `string/boolean`, числовой матч с локальными форматами для `number`).

3. **Rerank улучшает топ-1**

   * На спец-наборе из 15 вопросов “BM25-ловушка” (релевантность по смыслу, мало терминов) — доля попаданий **релевантной страницы в top-1 ≥ 60%** (vs baseline < 30%).

4. **Гибридный merge корректен**

   * На наборе 20 вопросов проверяется, что **в топ-3** присутствует релевантная страница (достаточно по одной) — **≥ 85%**.

5. **Кэш работает**

   * Повторный прогон того же набора вопросов даёт **`cache_retrieval_hits > 0`** и **`cache_rerank_hits > 0`** в логах (`logs/`), и end-to-end время повторного прогона **уменьшается** (порог условный, тест читает лог и сверяет рост счётчиков).

6. **Батчинг/конкурентность**

   * Зафиксирован параллелизм `≥ 8` одновременных задач (по логам оркестратора).

> Все тестовые пороги настроены под небольшой, но показательный корпус. Временные лимиты мягкие, упор — на **качество Retrieval** и **наличие кэша**.

---

## Запуск

```bash
# 1) ingest -> pages.jsonl
python scripts/run_all.py ingest --input ./corpus --out ./data/pages.jsonl
# 2) index
python scripts/run_all.py index --pages ./data/pages.jsonl --out_dense ./data/faiss.index --out_bm25 ./data/bm25.json
# 3) answer
python scripts/run_all.py answer --pages ./data/pages.jsonl --faiss ./data/faiss.index --bm25 ./data/bm25.json --questions ./data/questions.jsonl --out ./answers.json
```

---

## Что можно менять / что нельзя

* **Можно**: редактировать только

  * `index/build_dense.py`,
  * `retriever/hybrid.py`,
  * `retriever/rerank.py`,
  * `orchestrator/pipeline.py`,
  * `configs/default.yaml`.
* **Нельзя**: менять интерфейсы CLI (`scripts/run_all.py`), формат входа/выхода и имена файлов вывода.

---

## Подсказки (очень кратко)

* Нормируйте dense-вектора (косинус через dot product).
* В merge полезно **percentile-обрезка/порог** для “шумных” кандидатов.
* В rerank ускоряет батчинг и кэш; простая функция `score = sim(query, page_text)` со строгой нормировкой — лучше, чем рандом.
* В кэше используйте простой **LRU** из `functools.lru_cache` или свою очередь.

---

## Пример ожидаемого ответа (фрагмент)

```json
[
  {
    "question_id": 17,
    "answer": 1234.5,
    "sources": [{"document": "doc_fin_2024", "page": 12}]
  },
  {
    "question_id": 18,
    "answer": "Yes",
    "sources": [{"document": "doc_cert_001", "page": 3}]
  }
]
```

---

Если хочешь, я сразу добавлю эту задачу как `TASK_CODEX.md` в твой репозиторий (и короткие “заглушки” для α в конфиге и логов счётчиков), чтобы можно было мгновенно отдавать в CodeX.

